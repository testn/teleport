---
title: Server Access Getting Started Guide
description: Getting started with Teleport Server Access.
videoBanner: EsEvO5ndNDI
---

# Getting Started

Server Access involves managing your resources, configuring new clusters, and issuing commands through a CLI or programmatically to an API.

This guide introduces some of these common scenarios and how to interact with Teleport to accomplish them:

1. SSH into a cluster using Teleport.
2. Introspect the cluster using Teleport features.

<Admonition type="tip" title="Tip">
  This guide also demonstrates how to configure Teleport nodes using the **bastion pattern** so that only a single node can be accessed publicly.
</Admonition>

<Figure
  align="center"
  bordered
  caption="Teleport Bastion"
>
  ![Teleport Bastion](../../img/server-access/getting-started-diagram.svg)
</Figure>

## Prerequisites

- The Teleport Auth Service and Proxy Service, deployed on your own infrastructure or managed via Teleport Cloud.
- One host running your favorite Linux environment (such as Ubuntu 20.05, CentOS 8.0-1905, or Debian 10). This will serve as a Teleport Server Access node.
- Teleport (=teleport.version=) installed locally.

<Admonition type="tip" title="New Teleport users">
If you have not yet deployed the Teleport Auth Service and Proxy Service, learn how to do so by following one of our [getting started guides](../getting-started.mdx).
</Admonition>

(!docs/pages/includes/permission-warning.mdx!)

## Step 1/4. Install Teleport 

1. Create a new instance of your desired Linux distribution (such as Ubuntu 20.05, CentOS 8.0-1905, or Debian 10).

   This instance will be a private resource. Open port 22 so you can initially access, configure, and provision your instance. We'll configure and launch our instance, then demonstrate how to use the `tsh` tool and Teleport in SSH mode thereafter.

2. Install Teleport on your instance.

   <Tabs>
     <TabItem label="Amazon Linux 2/RHEL (RPM)">
      ```code
      $ sudo yum-config-manager --add-repo https://rpm.releases.teleport.dev/teleport.repo
      $ sudo yum install teleport

      # Optional:  Using DNF on newer distributions
      # $ sudo dnf config-manager --add-repo https://rpm.releases.teleport.dev/teleport.repo
      # $ sudo dnf install teleport
      ```
     </TabItem>

     <TabItem label="Debian/Ubuntu (DEB)">
      ```code
      $ curl https://deb.releases.teleport.dev/teleport-pubkey.asc | sudo apt-key add -
      $ sudo add-apt-repository 'deb https://deb.releases.teleport.dev/ stable main'
      $ sudo apt-get update
      $ sudo apt-get install teleport
      ```
     </TabItem>

     <TabItem label="Linux">
      ```code
      $ curl -O https://get.gravitational.com/teleport-v(=teleport.version=)-linux-amd64-bin.tar.gz
      $ tar -xzf teleport-v(=teleport.version=)-linux-amd64-bin.tar.gz
      $ cd teleport
      $ sudo ./install
      ```
     </TabItem>

     <TabItem label="ARMv7 (32-bit)">
      ```code
      $ curl -O https://get.gravitational.com/teleport-v(=teleport.version=)-linux-arm-bin.tar.gz
      $ tar -xzf teleport-v(=teleport.version=)-linux-arm-bin.tar.gz
      $ cd teleport
      $ sudo ./install
      ```
     </TabItem>

     <TabItem label="ARMv8 (64-bit)">
      ```code
      $ curl -O https://get.gravitational.com/teleport-v(=teleport.version=)-linux-arm64-bin.tar.gz
      $ tar -xzf teleport-v(=teleport.version=)-linux-arm64-bin.tar.gz
      $ cd teleport
      $ sudo ./install
      ```
     </TabItem>
   </Tabs>

   Next, we'll create a **join token** to add and start Teleport Server Access on the node.

## Step 2/4. Add a node to the cluster

1. Create a join token to add the node to your Teleport cluster. In run the following command, either on your Auth Service host (for self-hosted deployments) or on your local machine (for Teleport Cloud).

   ```code
   # Let's save the token to a file
   $ sudo tctl tokens add --type=node | grep -oP '(?<=token:\s).*' > token.file
   ```

   Each Teleport node can be configured into SSH mode and run as an enhanced SSH server. `--type=node` specifies that the Teleport node will act and join as an SSH server.

   `> token.file` indicates that you'd like to save the output to a file name `token.file`.

   <Admonition type="tip" title="Tip">
     This helps to minimize the direct sharing of tokens even when they are dynamically generated.
   </Admonition>

2. Now, open a new terminal and connect to the Teleport Auth Service.

   - On your node, save `token.file` to an appropriate, secure, directory you have the rights and access to read.

   - Start the node. Change `tele.example.com` to the address of your Teleport Proxy Service. For Teleport Cloud customers, use a tenant address such as `mytenant.teleport.sh`. Assign the `--token` flag to the path where you saved `token.file`.

   ```code
   # Join cluster
   $ sudo teleport start \
     --roles=node \
     --token=/path/to/token.file \
     --auth-server=tele.example.com:443
   ```

3. Create a user to access the Web UI through the following command:

   ```code
   $ sudo tctl users add tele-admin --roles=editor,access --logins=root,ubuntu,ec2-user
   ```

   This will generate an initial login link where you can set a password and set up Two-Factor Authentication for `tele-admin`.

   <Admonition type="note" title="Note">
     We've only given `tele-admin` the roles `editor` and `access` according to the *Principle of Least Privilege* (POLP).
   </Admonition>

4. You should now be able to view your Teleport node in Teleport Web interface after logging in as `tele-admin`:

   <Figure
     align="center"
     bordered
     caption="Both nodes in the Web UI"
   >
     ![Both nodes in the Web UI](../../img/server-access/teleport_ui.png)
   </Figure>

## Step 3/4. SSH into the server

Now, that we've got our cluster up and running, let's see how easy it is to connect to our node.

We can use `tsh` to SSH into the cluster:

1. On your local machine, log in through `tsh`, assigning the `--proxy` flag to the address of your Teleport Proxy Service:

   ```code
   # Log in through tsh
   $ tsh login --proxy=tele.example.com --user=tele-admin
   ```

   You'll be prompted to supply the password and second factor we set up previously.

2. `tele-admin` will now see something similar to:

   ```txt
   Profile URL:        https://tele.example.com:443
     Logged in as:       tele-admin
     Cluster:            tele.example.com
     Roles:              access, editor
     Logins:             root, ubuntu, ec2-user
     Kubernetes:         disabled
     Valid until:        2021-04-30 06:39:13 -0500 CDT [valid for 12h0m0s]
     Extensions:         permit-agent-forwarding, permit-port-forwarding, permit-pty
   ```

   In this example, `tele-admin` is now logged into the `tele.example.com` cluster through Teleport SSH.

3. `tele-admin` can now execute the following to find the cluster's `nodenames`. `nodenames` are used for establishing SSH connections:

   ```code
   # Display cluster resources
   $ tsh ls
   ```

   In this example, the bastion host node is located on the bottom line below:

   ```txt
   Node Name        Address        Labels                                 
   ---------------- -------------- -------------------------------------- 
   ip-172-31-35-170 ‚üµ Tunnel                                              
   ip-172-31-41-144 127.0.0.1:3022 env=example, hostname=ip-172-31-41-144 
   ```

4. `tele-admin` can SSH into the bastion host node by running the following command locally:

   ```code
   # Use tsh to ssh into a node
   $ tsh ssh root@ip-172-31-41-144
   ```

   Now, they can:

   - Connect to other nodes in the cluster by using the appropriate IP address in the `tsh ssh` command.
   - Traverse the Linux file system.
   - Execute desired commands.

   All commands executed by `tele-admin` are recorded and can be replayed in the Teleport Web UI.

   The `tsh ssh` command allows one to do anything they would if they were to SSH into a server using a third-party tool. Compare the two equivalent commands:

<Tabs>
  <TabItem label="tsh">
    ```code
    $ tsh ssh root@ip-172-31-41-144
    ```
  </TabItem>
  <TabItem label="ssh">
    ```code
    $ ssh -J tele.example.com root@ip-172-31-41-144
    ```
  </TabItem>
</Tabs>

## Step 4/4. Use tsh and the unified resource catalog to introspect the cluster

1. Now, `tele-admin` has the ability to SSH into other nodes within the cluster, traverse the Linux file system, and execute commands.

   - They have visibility into all resources within the cluster due to their defined and assigned roles.
   - They can also quickly view any node or grouping of nodes that have been assigned a particular label.

2. Execute the following command within your bastion host console:

   ```code
   # List nodes
   $ sudo tctl nodes ls
   ```

   It displays the unified resource catalog with all queried resources in one view: 
   
   ```txt
   Nodename         UUID                                 Address        Labels                                
   ---------------- ------------------------------------ -------------- ------------------------------------- 
   ip-172-31-35-170 4980899c-d260-414f-9aea-874feef71747                                                      
   ip-172-31-41-144 f3d2a65f-3fa7-451d-b516-68d189ff9ae5 127.0.0.1:3022 env=example,hostname=ip-172-31-41-144                            
   ```

3. Note the "Labels" column on the farthest side. `tele-admin` can query all resources with a shared label using the command:

   ```code
   # Query all Nodes with a label
   $ tsh ls env=example
   ```

   Customized labels can be defined in your `teleport.yaml` configuration file or during node creation.

   This is a convenient feature that allows for more advanced queries. If an IP address changes, for example, an admin can quickly find the current node with that label since it remains unchanged.

4. `tele-admin` can also execute commands on all nodes that share a label, vastly simplifying repeated operations. For example, the command:

   ```code
   # Run the ls command on all nodes with a label
   $ tsh ssh root@env=example ls
   ``` 

   will execute the `ls` command on each node and display the results in your terminal.

## Conclusion

<Admonition type="tip" title="Note">
   We previously configured our Linux instance to leave port `22` open to easily configure and install Teleport. Feel free to compare Teleport SSH to your usual `ssh` commands.

   If you'd like to further experiment with using Teleport according to the bastion pattern:

   - Close port `22` on your private Linux instance now that your Teleport node is configured and running.
   - For self-hosted deployments, optionally close port `22` on your bastion host.
   - You'll be able to fully connect to the private instance and, for self-hosted deployments, the bastion host, using `tsh ssh`.
</Admonition>

To recap, this guide described:

1. How to set up and add an SSH node to a cluster.
2. Connect to the cluster using `tsh` to manage and introspect resources.

Feel free to shut down, clean up, and delete your resources, or use them in further Getting Started exercises.

## Next steps

- Learn more about Teleport `tsh` through the [reference documentation](../setup/reference/cli.mdx#tsh-ssh).
- Learn more about [Teleport nodes](../architecture/nodes.mdx#connecting-to-nodes)
- For a complete list of ports used by Teleport, read [The Admin Guide](../setup/reference/networking.mdx).

## Resources
- [Setting Up an SSH Bastion Host](https://goteleport.com/blog/ssh-bastion-host/)
- [Announcing Teleport SSH Server](https://goteleport.com/blog/announcing-teleport-ssh-server/)
- [How to SSH properly](https://goteleport.com/blog/how-to-ssh-properly/)
- Consider whether [OpenSSH or Teleport SSH](https://goteleport.com/blog/openssh-vs-teleport/) is right for you.
- [Labels](../setup/admin/labels.mdx)
